# AI Research Processor

**AI-Powered Scientific Hypothesis Generator for Biomedical Research**

A comprehensive research tool with an intuitive graphical interface that scrapes scientific literature, generates embeddings, and uses AI to generate novel research hypotheses for any biomedical research topic. This system accelerates scientific discovery by analyzing vast amounts of research data and proposing testable hypotheses.

## ğŸš€ Quick Start

## We now have a website!
You can find AHGA at ahga.it.com

#However if you prefer local usage, we also have a local installation option:

### **Windows Users (Recommended)**
```bash
# 1. First time setup (run once)
install.bat

# 2. Launch GUI (run anytime)
python src/interfaces/gui_main.py
```

### **Manual Installation**
```bash
# Create a virtual environment
python -m venv .venv

# Install dependencies
pip install -r requirements.txt

# Launch GUI
python src/interfaces/gui_main.py
```

## ğŸ“ Project Structure

The codebase is now organized into logical modules for better maintainability:

```
AHG-UBR5/
â”œâ”€â”€ src/                          # Main source code
â”‚   â”œâ”€â”€ core/                     # Core functionality
â”‚   â”‚   â”œâ”€â”€ chromadb_manager.py   # Vector database management
â”‚   â”‚   â”œâ”€â”€ processing_config.py  # Configuration settings
â”‚   â”‚   â””â”€â”€ network_fix.py        # Network connectivity fixes
â”‚   â”œâ”€â”€ scrapers/                 # Data scraping modules
â”‚   â”‚   â”œâ”€â”€ pubmed_scraper_json.py
â”‚   â”‚   â”œâ”€â”€ process_xrvix_dumps_json.py
â”‚   â”‚   â”œâ”€â”€ semantic_scholar_scraper.py
â”‚   â”‚   â””â”€â”€ xrvix_downloader.py
â”‚   â”œâ”€â”€ ai/                       # AI and hypothesis generation
â”‚   â”‚   â”œâ”€â”€ enhanced_rag_with_chromadb.py
â”‚   â”‚   â”œâ”€â”€ hypothesis_tools.py
â”‚   â”‚   â””â”€â”€ optimized_prompts.py
â”‚   â”œâ”€â”€ utils/                    # Utility modules
â”‚   â”‚   â””â”€â”€ citation_mapping_utils.py # Citation analysis tools
â”‚   â””â”€â”€ interfaces/               # User interfaces
â”‚       â”œâ”€â”€ main.py               # Terminal interface
â”‚       â””â”€â”€ gui_main.py           # GUI interface
â”œâ”€â”€ scripts/                      # Command-line tools
â”‚   â””â”€â”€ analyze_citations.py     # Citation analysis script
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ keys.json                 # API keys
â”‚   â”œâ”€â”€ search_keywords_config.json
â”‚   â””â”€â”€ critique_config.json
â”œâ”€â”€ install.bat                   # Installation script
â”œâ”€â”€ run.bat                       # GUI launcher script
â””â”€â”€ run_on_terminal.bat           # Terminal launcher script
â”œâ”€â”€ data/                         # Data directories
â”œâ”€â”€ docs/                         # Documentation
â””â”€â”€ hypothesis_export/            # Export files
```

## ğŸ–¥ï¸ GUI Interface

The system now features a modern **Tkinter-based graphical interface** with tabbed organization:

### **Main Tabs**
- **ğŸ“š Paper Scraping & Processing** - Collect data from multiple sources
- **ğŸ—„ï¸ Vector Database Management** - Manage ChromaDB storage
- **âš™ï¸ Settings & Config** - System status and configuration
- **ğŸ§  Hypothesis Generation** - AI-powered hypothesis creation
- **ğŸ“– Tutorial** - Comprehensive data pipeline guide

### **Key Features**
- **Scrollable Interface** - All tabs support scrolling for better navigation
- **Resizable Text Windows** - Click and drag to resize output areas
- **Real-time Progress** - Live updates and status indicators
- **Threading Support** - GUI remains responsive during long operations
- **Error Handling** - User-friendly error messages and suggestions

## ğŸ“š Complete Installation Tutorial

### **Step 1: Prerequisites**

Before installing, ensure you have:

- **Python 3.13+** installed on your system
- **Internet connection** for downloading dependencies and data
- **Google AI API key** (optional but recommended for full functionality)
- **At least 2GB free disk space** for data storage

#### **Installing Python (if needed)**
1. Go to [python.org/downloads](https://www.python.org/downloads/)
2. Download Python 3.13 or newer
3. **IMPORTANT**: During installation, check "Add Python to PATH"
4. Complete the installation

### **Step 2: Download the Software**

1. **Clone or download** this repository to your computer
2. **Navigate** to the AI Research Processor folder in your file explorer
3. **Open Command Prompt** or PowerShell in this folder

### **Step 3: Installation (Windows - Easy Method)**

#### **Option A: Automatic Installation (Recommended)**
```bash
# Double-click install.bat or run from command prompt:
install.bat
```

**What the installer does:**
- âœ… Checks Python installation
- âœ… Creates virtual environment (`.venv`)
- âœ… Installs all required packages
- âœ… Creates data directory structure
- âœ… Checks API configuration
- âœ… Provides setup guidance

#### **Option B: Manual Installation**
```bash
# Create virtual environment
python -m venv .venv

# Activate virtual environment
.venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create data directories
mkdir data
mkdir data\embeddings
mkdir data\embeddings\xrvix_embeddings
mkdir data\embeddings\xrvix_embeddings\biorxiv
mkdir data\embeddings\xrvix_embeddings\medrxiv
mkdir data\embeddings\xrvix_embeddings\pubmed
mkdir data\embeddings\xrvix_embeddings\semantic_scholar
mkdir data\logs
mkdir data\scraped_data
mkdir data\vector_db
mkdir data\backups
```

### **Step 4: API Configuration**

A google API and gemini API key is **required**, and an NCBI API key is *reccommended* if you want to scrape from pubmed.

1. **Get API Key**: Visit [Google AI Studio](https://aistudio.google.com/)
2. **Create keys.json**: In the AI Research Processor folder, create a file called `keys.json`:
```json
{
  "ncbi_api_key": "your_api_key_here",
  "GOOGLE_API_KEY": "your_api_key_here",
  "GEMINI_API_KEY": "your_api_key_here"
}
```

### **Step 5: Running the Software**

run 
```bash
run.bat
```
#### **Legacy Command Line Interface**
run 
```bash
run_on_terminal.bat
```

### **Step 6: First-Time Usage with GUI**

When you first run the GUI:

1. **Start with Tutorial Tab** - Read the comprehensive data pipeline guide
2. **Check System Status** - Go to Settings â†’ Data Status
3. **Collect Data** - Use Paper Scraping â†’ Full Scraper (start with option 1)- **Note that this can take a while, up to many hours- but it only needs to be done once.**
4. **Configure Keywords** - Enter your research interests or use defaults
5. **Load Data** - Go to Vector Database â†’ Load Embeddings
6. **Generate Hypotheses** - Use Hypothesis Generation â†’ Generate Hypothesess

## ğŸ“‹ GUI Interface Guide

### **ğŸ“š Paper Scraping & Processing Tab**

**Full Scraper Subtab**
- Scrapes all sources (PubMed, preprints, Semantic Scholar)
- Customizable keywords for both PubMed and Semantic Scholar
- Configurable PubMed result limits
- Automatic preprint dump downloading

**Journal Articles Only Subtab**
- Focuses on peer-reviewed literature
- PubMed + Semantic Scholar scraping
- Unified keyword configuration

**Preprints Only Subtab**
- Processes Biorxiv and Medrxiv dumps
- Automatic dump downloading if needed
- Status checking for available dumps

**Generate Embeddings Subtab**
- Embedding generation status
- Information about automatic embedding creation

### **ğŸ—„ï¸ Vector Database Management Tab**

**Load Embeddings Subtab**
- Loads processed data into ChromaDB
- Real-time progress monitoring
- Automatic data source detection
- Reload confirmation for existing data

**Show ChromaDB Data Subtab**
- Database statistics and document counts
- Source breakdown analysis
- Collection information
- Refresh functionality

**Clear ChromaDB Data Subtab**
- Safe data clearing with confirmation
- Warning messages for destructive operations
- Status updates during clearing

### **âš™ï¸ Settings & Config Tab**

**Data Status Subtab**
- Overview of all data sources
- File size and document count information
- ChromaDB status checking
- Recommendations for next steps

**Configurations Subtab**
- Current system settings display
- Processing configuration information
- Database settings overview

### **ğŸ§  Hypothesis Generation Tab**

**Generate Hypotheses Subtab**
- Prerequisites checking
- ChromaDB data validation
- Enhanced RAG system launch
- Status monitoring

**Test Run Subtab**
- Customizable test queries for any research topic
- Prerequisites validation
- Quick demonstration mode

### **ğŸ“– Tutorial Tab**

**Comprehensive Guide Including:**
- Data pipeline overview (4 stages)
- Detailed stage explanations
- Recommended workflow
- Configuration tips
- Troubleshooting guide
- Advanced features
- Getting help resources

### **ğŸ“Š Citation Analysis Tab**

**Citation Analysis Tools:**
- Analyze citation patterns across hypotheses
- Export citation data to CSV for further analysis
- Track citation cache keys for easy mapping
- Generate comprehensive citation reports

**Command Line Usage:**
```bash
# Analyze latest hypothesis export
python scripts/analyze_citations.py

# Analyze specific export
python scripts/analyze_citations.py hypothesis_export/Hypothesis_Export_09202025-2043
```

**Programmatic Usage:**
```python
from src.utils.citation_mapping_utils import CitationMappingUtils

# Load citation data
utils = CitationMappingUtils("hypothesis_export/Hypothesis_Export_09202025-2043")

# Get citations for hypothesis #5
citations = utils.get_hypothesis_citations(5)

# Create summary table
summary_df = utils.create_citation_summary_table()

# Export to CSV
utils.export_citation_mapping_to_csv()
```

## ğŸ—ï¸ System Architecture

### Core Components

- **`gui_main.py`** - Main GUI interface with tabbed organization
- **`main.py`** - Legacy command line interface
- **`enhanced_rag_with_chromadb.py`** - Advanced RAG system with hypothesis generation
- **`chromadb_manager.py`** - Vector database management
- **`pubmed_scraper_json.py`** - PubMed literature scraping
- **`process_xrvix_dumps_json.py`** - Preprint processing (Biorxiv, Medrxiv)
- **`semantic_scholar_scraper.py`** - Semantic Scholar API scraping
- **`hypothesis_tools.py`** - AI hypothesis generation tools
- **`citation_mapping_utils.py`** - Citation analysis and mapping utilities

### Data Sources

1. **PubMed** - Peer-reviewed journal articles
2. **Biorxiv** - Biology preprint server
3. **Medrxiv** - Medicine preprint server  
4. **Semantic Scholar API** - Comprehensive academic literature search

## ğŸ“ Data Structure

```
data/
â”œâ”€â”€ backups/                    # Backup files
â”‚   â””â”€â”€ embeddings_backup.zip   # Embeddings backup
â”œâ”€â”€ embeddings/                 # Generated embeddings
â”‚   â””â”€â”€ xrvix_embeddings/      # Main embeddings directory
â”‚       â”œâ”€â”€ biorxiv/           # Biorxiv preprint embeddings
â”‚       â”œâ”€â”€ medrxiv/           # Medrxiv preprint embeddings
â”‚       â”œâ”€â”€ pubmed/            # PubMed journal article embeddings
â”‚       â””â”€â”€ semantic_scholar/   # Semantic Scholar API scraped embeddings
â”œâ”€â”€ logs/                      # Log files
â”‚   â”œâ”€â”€ paper_processing.log   # Paper processing logs
â”‚   â””â”€â”€ semantic_scholar_scraping.log # Semantic Scholar API scraping logs
â”œâ”€â”€ scraped_data/              # Raw scraped data
â”‚   â””â”€â”€ paperscraper_dumps/    # Paperscraper dump files
â””â”€â”€ vector_db/                 # Vector database storage
    â””â”€â”€ chroma_db/            # ChromaDB persistent storage
```

## ğŸ› ï¸ Configuration

### Processing Configuration
- **Batch Sizes** - Configurable processing batch sizes
- **Rate Limiting** - API rate limit management
- **Parallel Processing** - Multi-threaded data processing
- **Memory Management** - Efficient memory usage

### API Configuration
- **Google AI Studio** - Gemini model configuration
- **Semantic Scholar** - API key and rate limiting
- **PubMed** - E-utilities API configuration

## ğŸ“ˆ Performance

### Optimization Features
- **Memory Profiling** - Efficient memory usage
- **Progress Monitoring** - Real-time performance tracking
- **Error Recovery** - Automatic retry and recovery mechanisms

### Scalability
- **Batch Processing** - Handles large datasets efficiently
- **Persistent Storage** - ChromaDB for fast data retrieval
- **Modular Design** - Easy to extend and modify
- **API Integration** - Seamless integration with external APIs

## ğŸ› Troubleshooting

### Common Issues

1. **"No ChromaDB collections found"**
   - Solution: Run "Load Embeddings" first to populate the database

2. **"No preprint dumps found"**
   - Solution: The system will automatically download dumps when needed
   - Ensure you have internet connection

3. **"Keywords not saved"**
   - Solution: Check file permissions in the project directory
   - Try running as administrator if needed

4. **"Import errors"**
   - Solution: Install missing dependencies: `pip install pandas paperscraper chromadb`
   - Check Python version compatibility

5. **"Long processing times"**
   - Solution: This is normal for large datasets
   - Use smaller PubMed limits for faster initial runs
   - Monitor progress in output areas

### Log Files
- Check `data/logs/` for detailed error information
- Monitor processing status in real-time through GUI output areas

## ğŸ¤ Contributing

### Development Setup
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

### Code Style
- Follow PEP 8 guidelines
- Add docstrings to functions
- Include type hints where appropriate
- Write comprehensive tests

## ğŸ“„ License

See the LICENSE file for details.

## ğŸ™ Acknowledgments

- **Google AI Studio** - Gemini model access
- **Semantic Scholar** - Research paper API
- **ChromaDB** - Vector database technology
- **Tkinter** - GUI framework
- **Paperscraper** - Preprint data access

## ğŸ“ Support

For questions, issues, or contributions:
- Create an issue in the repository
- Review log files for error details
- Check the Tutorial tab in the GUI for comprehensive guidance
- Review the documentation and troubleshooting sections
- Contact me at marcellino.rau@gmail.com for any questions or info!

---
